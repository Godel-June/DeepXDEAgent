import os
import json
import random
from tqdm import tqdm
from prettytable import PrettyTable 
from termcolor import cprint
from pptree import Node
import google.generativeai as genai
from openai import OpenAI
from pptree import *
from prompts.prompts import *
import logging
import re
from matplotlib import pyplot as plt

from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_deepseek import ChatDeepSeek
from langchain_community.chat_models import ChatOpenAI
import sys
from langchain.chains import RetrievalQA

class Agent:
    def __init__(self, instruction, role, model_info=None, examplers=None, img_path=None, is_recorded=False):
        self.instruction = instruction
        self.role = role
        self.model_info = model_info if model_info is not None else os.getenv('CHAT_MODEL', 'gpt-4o-mini')
        self.img_path = img_path
        self.is_recorded = is_recorded

        if self.model_info == 'gemini-pro':
            self.model = genai.GenerativeModel('gemini-pro')
            self._chat = self.model.start_chat(history=[])
        elif self.model_info in ['gpt-3.5', 'gpt-4', 'gpt-4o', 'gpt-4o-mini', 'deepseek-chat']:
            # self.client = OpenAI(api_key=model_info['deepseek_api_key'], 
            #                      base_url=model_info['deepseek_base_url']) if self.model_info=='deepseek-chat' else OpenAI(api_key=model_info['openai_api_key'])
            self.client = OpenAI(api_key=os.environ['DEEPSEEK_API_KEY'], 
                                 base_url=os.environ['DEEPSEEK_BASE_URL']) if self.model_info=='deepseek-chat' else OpenAI(api_key=os.environ['OPENAI_API_KEY'])
            self.messages = [
                {"role": "system", "content": instruction},
            ]
            if examplers is not None:
                for exampler in examplers:
                    self.messages.append({"role": "user", "content": exampler['question']})
                    self.messages.append({"role": "assistant", "content": exampler['answer'] + "\n\n" + exampler['reason']})
        if self.is_recorded:
            self.record_path = os.path.join(os.getenv('OUTPUT_DIR'), f'{self.role}_record.txt')
            with open(self.record_path, 'w', encoding='utf-8') as f:
                for item in self.messages:
                    f.write("#"*5 + item['role'] + ':'+ '\n' + item['content'] + '\n\n')

    def chat(self, message, img_path=None, chat_mode=True):
        if self.model_info == 'gemini-pro':
            for _ in range(10):
                try:
                    response = self._chat.send_message(message, stream=True)
                    responses = ""
                    for chunk in response:
                        responses += chunk.text + "\n"
                    return responses
                except:
                    continue
            return "Error: Failed to get response from Gemini."

        elif self.model_info in ['gpt-3.5', 'gpt-4', 'gpt-4o', 'gpt-4o-mini', 'deepseek-chat']:
            self.messages.append({"role": "user", "content": message})
            
            if self.model_info == 'gpt-3.5':
                model_name = "gpt-3.5-turbo"
            elif self.model_info == 'deepseek-chat':
                model_name = "deepseek-chat"
            else:
                model_name = "gpt-4o-mini"

            response = self.client.chat.completions.create(
                model=model_name,
                messages=self.messages,
                temperature=0,  # è´ªå©ªç­–ç•¥ï¼Œå›ºåŒ–å›ç­”
                top_p=0.8,
                max_tokens=5120,
                extra_body={
                    "repetition_penalty": 1.05,
                },
            )
            
            if self.is_recorded:
                with open(self.record_path, 'a', encoding='utf-8') as f:
                    f.write("user: " + '\n' + message + '\n\n')
                    f.write("assistant: " + '\n' + response.choices[0].message.content + '\n\n')
            
            think_content, answer_content = self.response_split(response.choices[0].message.content)
            self.messages.append({"role": "assistant", "content": answer_content})
            return think_content, answer_content
        
    def response_split(self, content: str) -> tuple:
        match = re.search(r'<think>(.*?)</think>(.*)', content, re.DOTALL)
        if match:
            think_content = match.group(1).strip()
            answer_content = match.group(2).strip().replace("json", "").replace("```", '')
            return think_content, answer_content
        else:
            return "æœªæ‰¾åˆ°æœ‰æ•ˆçš„<think>æ ‡ç­¾", content

    def temp_responses(self, message, img_path=None):
        if self.model_info in ['gpt-3.5', 'gpt-4', 'gpt-4o', 'gpt-4o-mini']:      
            self.messages.append({"role": "user", "content": message})
            
            temperatures = [0.0]
            
            responses = {}
            for temperature in temperatures:
                if self.model_info == 'gpt-3.5':
                    model_info = 'gpt-3.5-turbo'
                else:
                    model_info = 'gpt-4o-mini'
                response = self.client.chat.completions.create(
                    model=model_info,
                    messages=self.messages,
                    temperature=temperature,
                )
                
                responses[temperature] = response.choices[0].message.content
                
            return responses
        
        elif self.model_info == 'gemini-pro':
            response = self._chat.send_message(message, stream=True)
            responses = ""
            for chunk in response:
                responses += chunk.text + "\n"
            return responses


def determine_difficulty(question, difficulty, Debug=False):
    if difficulty != 'adaptive':
        return difficulty
    
    # difficulty_prompt = f"""Now, given the differential equation as below, you need to decide the difficulty/complexity of it:\n{question}.\n\nPlease indicate the difficulty/complexity of solving the differential equation with DeepXDE using among below options:\n1) easy: a single agent using DeepXDE can output an answer.\n2) medium: a single agent with RAG using DeepXDE can output an answer.\n3) â€˜difficult: multiple agent with RAG using DeepXDE can work together to make final answer."""
    difficulty_prompt = build_difficulty_prompt(question)
    
    math_agent = Agent(instruction='You are a math expert who conducts initial assessment and your job is to decide the difficulty/complexity of solving differential equation with DeepXDE.', role='math expert', moded_info='deepseek')
    # math_agent.chat('You are a math expert who conducts initial assessment and your job is to decide the difficulty/complexity of the math query.')
    response = math_agent.chat(difficulty_prompt)
    
    if Debug:
        logging.info(f"Difficulty prompt: {difficulty_prompt}")
        logging.info(f"Difficulty response: \n{response}")

    # 1) Easy, 2) Medium, or 3) Difficult
    if 'easy' in response.lower() or '1)' in response.lower():
        return 'easy'
    elif 'medium' in response.lower() or '2)' in response.lower():
        return 'medium'
    elif 'difficult' in response.lower() or '3)' in response.lower():
        return 'difficult'
    
def parse_tool_params(tool_params: dict, previous_steps_info: list) -> dict:
    """
    è§£æå·¥å…·å‚æ•°ï¼Œè¿”å›ä¸€ä¸ª kwargs å­—å…¸ï¼Œæ–¹ä¾¿åç»­ç›´æ¥é€šè¿‡ func(**kwargs) è°ƒç”¨å·¥å…·ã€‚

    å¦‚æœæŸä¸ªå‚æ•°çš„å€¼ä¸ºå­—ç¬¦ä¸²ä¸”ç¬¦åˆ "Step_n_m" æ ¼å¼ï¼Œåˆ™ç”¨ previous_steps_info ä¸­å¯¹åº”æ­¥éª¤çš„è¿”å›å€¼æ›¿æ¢ï¼Œ
    å¦åˆ™ç›´æ¥ä½¿ç”¨åŸå§‹å€¼ï¼ˆJSON è§£ææ—¶å·²ç»ä¿è¯äº†æ•°å€¼ã€boolç­‰ç±»å‹æ­£ç¡®ï¼‰ã€‚

    :param tool_params: å·¥å…·æ‰€éœ€å‚æ•°çš„å­—å…¸ï¼Œæ ¼å¼å¦‚ï¼š
        {
            "param_name": {
                "type": "ç±»å‹æè¿°",
                "value": "Step_n_m" æˆ–ç›´æ¥å€¼
            },
            ...
        }
    :param previous_steps_info: ä¸€ä¸ªåˆ—è¡¨ï¼Œå…¶ä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å·¥å…·åç§°ã€å‚æ•°ä¿¡æ¯å’Œè¿”å›ç»“æœï¼Œå¦‚ï¼š
        {
            "step": æ­¥æ•°ï¼ˆä» 1 å¼€å§‹ï¼‰,
            "tool_name": "å·¥å…·A",
            "params": "å‚æ•°ä¿¡æ¯"
            "return1": {"type": "...", "value": "value1"},
            "return2": {"type": "...", "value": "value2"},
            ...
        }
    :return: kwargs å­—å…¸ï¼Œå…¶ä¸­æ¯ä¸ªé”®å¯¹åº”ä¸€ä¸ªå‚æ•°åï¼Œå€¼ä¸ºè§£æåçš„å‚æ•°å€¼
    """
    kwargs = {}
    step_pattern = re.compile(r"Step_(\d+)_(\d+)")  # åŒ¹é… "Step_n_m" æ ¼å¼

    for param_name, param_info in tool_params.items():
        param_value = param_info["value"]

        # ä»…å½“å‚æ•°å€¼ä¸ºå­—ç¬¦ä¸²æ—¶æ£€æŸ¥æ˜¯å¦ä¸º "Step_n_m" æ ¼å¼
        if isinstance(param_value, str):
            match = step_pattern.fullmatch(param_value)
            if match:
                step_index = int(match.group(1)) - 1  # è½¬æ¢ä¸ºåˆ—è¡¨ç´¢å¼•ï¼ˆä» 0 å¼€å§‹ï¼‰
                return_key = f"return{int(match.group(2))}"  # æ„é€ è¿”å›å€¼é”®ï¼Œä¾‹å¦‚ "return1"
                try:
                    param_value = previous_steps_info[step_index][return_key]["value"]
                except (IndexError, KeyError):
                    raise ValueError(f"Invalid reference: {param_value} not found in previous_steps_info")

        kwargs[param_name] = param_value

    return kwargs

def execute_tool(Tool_Dict: dict, tool_name: str, kwargs: dict) -> dict:
    """
    æ‰§è¡Œå·¥å…·ï¼Œè¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å·¥å…·çš„è¿”å›ç»“æœã€‚
    :param tool_name: å¾…è°ƒç”¨çš„å·¥å…·åç§°
    :param kwargs: å¾…è°ƒç”¨å·¥å…·çš„å‚æ•°å­—å…¸
    :return:
    """
    tool_func = Tool_Dict[tool_name]
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·¥å…·ï¼Œè¾“å‡ºé”™è¯¯
    if tool_func is None:
        raise ValueError(f"Tool {tool_name} Not Found in Toolset")

    return_value = tool_func(**kwargs)

    # æ ¹æ®è¿”å›å€¼æ˜¯å¦ä¸ºå…ƒç»„ï¼Œç”Ÿæˆå¯¹åº”çš„ç»“æœå­—å…¸
    if isinstance(return_value, tuple):
        result_dict = {}
        for i, val in enumerate(return_value):
            if isinstance(val, list):
                result_dict[f"return{i + 1}"] = {
                    "type": [type(item).__name__ for item in val],
                    "value": val
                }
            else:
                result_dict[f"return{i + 1}"] = {"type": type(val).__name__, "value": val}
    elif isinstance(return_value, list):
        result_dict = {"return1": {"type": [type(item).__name__ for item in return_value], "value": return_value}}
    else:
        result_dict = {"return1": {"type": type(return_value).__name__, "value": return_value}}

    return result_dict
    
def process_easy_query(question, instruction, config, Debug=False):
    math_agent = Agent(instruction=instruction.strip(), role='math expert', is_recorded=True)
    task_split_prompt = build_query_split_prompt(question, config['tools_info_dict'])
    if Debug:
        logging.info("\n" + "#"*10 + " 1 ä»»åŠ¡è§„åˆ’ " + "#"*10)
        logging.info("planning agent å·²åˆ›å»º")
        logging.info(f"Task split prompt: \n{task_split_prompt}")
    task_split_think, task_split_answer = math_agent.chat(task_split_prompt)
    # è§£æä»»åŠ¡åˆ†è§£ç»“æœï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
    task_split_info = json.loads(task_split_answer)
    # TODO: logging info è¾“å‡ºä»»åŠ¡æ‹†åˆ†ä¿¡æ¯
    if Debug:
        logging.info(f"\nä»»åŠ¡è§„åˆ’å®Œæˆï¼š")
        logging.info(f"ğŸ¤” ä»»åŠ¡æ‹†åˆ†\nğŸ§  ä»»åŠ¡æ‹†åˆ†æ€è€ƒè¿‡ç¨‹ï¼š\n{task_split_think}")
        # logging.info(f"Task split think: \n{task_split_think}")
        logging.info(f"Task split answer: \n{task_split_answer}")
        logging.info(f"ä»»åŠ¡æ‹†åˆ†ç»“æœï¼š\n{task_split_info}")
        
        logging.info("\n" + "#"*10 + " 2 å·¥å…·è°ƒç”¨ " + "#"*10)
        logging.info("å·¥å…· agent åˆ›å»ºï¼š")
    
    implement_agent = Agent(instruction=config['implement_prompt'].strip().format(user_input=config['user_input']), role='Implement Agent', is_recorded=True)
    # é¡ºåºå¤„ç†æ¯ä¸ªä»»åŠ¡
    previous_steps_info = []  # å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨ï¼Œç”¨äºå‚æ•°ä¼ é€’
    for idx, task_info in enumerate(task_split_info):
        # å¾—åˆ°å½“å‰å¾…è°ƒç”¨å·¥å…·
        tool_name = task_info["tool_name"]
        # å¾—åˆ°å½“å‰ä»»åŠ¡æè¿°
        task_desc = task_info["reasoning"]
        # å¾—åˆ°å½“å‰å·¥å…·è¯¦æƒ…
        tool_info = [item for item in config['tools_info_dict'].items() if item[0] == tool_name]
        tool_info = tool_info[0] if tool_info else None
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·¥å…·ä¿¡æ¯ï¼Œè¾“å‡ºé”™è¯¯ä¿¡æ¯
        if not tool_info:
            # st.error("âš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            print(f"\nâš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚\n")
            logging.error(f"âš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            raise ValueError(f"Invalid tool name: {tool_name}")
        # print(f"\nå·²åœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°å·¥å…·ï¼š{tool_name}\n")
        logging.info("#"*5 + f" å·²åœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°å·¥å…·ï¼š{tool_name}")
        
        # ç¡®å®šå·¥å…·å‚æ•°
        tool_params_prompt = determine_tool_params_prompt(tool_info, previous_steps_info)
        tool_params_think, tool_params_answer = implement_agent.chat(tool_params_prompt)
        # å·¥å…·è§£æ
        tool_params = json.loads(tool_params_answer)
        tool_params_kwargs = parse_tool_params(tool_params, previous_steps_info)
        if Debug:
            logging.info(f"Tool info: {tool_info}")
            logging.info(f"\nğŸ› ï¸ å·¥å…·é€‰æ‹©ï¼š{tool_name}")
            logging.info(f"\tTool params prompt: \n{tool_params_prompt}")
            # logging.info(f"Tool params answer: \n{tool_params_answer}")
            # logging.info(f"Tool params: {tool_params}")
            # logging.info(f"Tool params kwargs: {tool_params_kwargs}")
            
            logging.info(f"\tğŸ§  ç¡®å®šå·¥å…·å‚æ•°ï¼š\n{tool_params_think}")
            logging.info(f"\tå‚æ•°ï¼š\n{tool_params_answer}")
            logging.info(f"\tè¾“å‡ºå‚æ•° json: {tool_params}")
            logging.info(f"\tå‚æ•°è§£æç»“æœï¼š{tool_params_kwargs}")
            
            logging.info(f"\nå·¥å…·{tool_name}å‚æ•°è§£æå®Œæˆ")
            # logging.info(f"å·¥å…·{tool_name}å‚æ•°ï¼š{tool_params_kwargs}")
            
        # æ‰§è¡Œå·¥å…·
        tool_result = execute_tool(config['Tool_Dict'], tool_name, tool_params_kwargs)
        
        # è®°å½•å·¥å…·æ‰§è¡Œç»“æœ
        previous_steps_info.append({
            "step": idx+1,
            "tool_name": tool_name,
            "params": tool_params_kwargs,
            **tool_result
        })
        
        if Debug:
            logging.info(f"\nå·¥å…· {tool_name} æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç»“æœï¼š\n{tool_result}")
            logging.info(f"\nâœ… æ‰§è¡Œç»“æœï¼š")
            figure_flag = False
            for id, (key, item) in enumerate(tool_result.items()):
                if isinstance(item["value"], plt.Figure):
                    figure_flag = True
                    # plt.plot(item["value"])  # æ˜¾ç¤º Matplotlib å›¾åƒ
                    # plt.show()
                    plt.figure(item["value"].figure)  # æŒ‡å®šè¦æ˜¾ç¤ºçš„å›¾å½¢
                    # plt.show()  # æ˜¾ç¤º Matplotlib å›¾åƒ
                    logging.info(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼šå›¾åƒå·²å±•ç¤º")
                    plt.savefig(os.path.join(os.getenv('OUTPUT_DIR'), f"{tool_name}_{id+1}.png"))
                else:
                    print(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼š{item['value']}")  # å…¶ä»–ç±»å‹ç›´æ¥è¾“å‡º
                    logging.info(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼š\n{item['value']}")
            if figure_flag:
                plt.show()  # æ˜¾ç¤º Matplotlib å›¾åƒ
            # logging.info(f"\n{tool_name}æ‰§è¡Œå®Œæˆ")
            logging.info(f"å½“å‰è®°å½•çš„æ‰€æœ‰å·¥å…·ç»“æœï¼š\n{previous_steps_info}")

    logging.info("\næ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ")
    return previous_steps_info, math_agent.messages, implement_agent.messages


def process_medium_query(question, config, Debug=False):
    # retrieval module
    pde_problem_directory = "./database/pde_problem_faiss"
    pde_case_directory = "./database/pde_case_faiss"
    # local_model_path = "tbs17/MathBERT"
    local_model_path = r"C:\Users\jermain\.cache\huggingface\hub\models--tbs17--MathBERT\snapshots\e26235ccf2b14614ef278b19caa44fdb5dcf050f"
    pde_problem_vectordb = FAISS.load_local(pde_problem_directory, HuggingFaceEmbeddings(model_name=local_model_path),allow_dangerous_deserialization=True)
    pde_case_vectordb = FAISS.load_local(pde_case_directory, HuggingFaceEmbeddings(model_name=local_model_path),allow_dangerous_deserialization=True)
    
    chat_model_name = config['model_info'].get('CHAT_MODEL', '')
    temperature = 0.0
    if chat_model_name.startswith("deepseek"):
        chat_model = ChatDeepSeek(model=chat_model_name, temperature=temperature)
    elif chat_model_name.lower().startswith("gpt"):
        chat_model = ChatOpenAI(model=chat_model_name, temperature=temperature)
    else:
        print('the chat model is not identified, only the model start with deepseek and gpt can be identified.')
        sys.exit()
    
    # k = config['searchdocs']
    k = 2
    retriever = pde_problem_vectordb.as_retriever(search_type="similarity", search_kwargs={"k": k})
    qa_interface = RetrievalQA.from_chain_type(
        llm=chat_model,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    
    prompt_translate = build_standard_format_prompt(question)
    rsp = qa_interface(prompt_translate)
    user_input = rsp['result']
    # å°†å…¶æå–å‡ºæ¥
    match = re.search(r'<answer>(.*?)</answer>(.*)', user_input, re.DOTALL)
    if match:
        user_input = match.group(1).strip().replace("json", "").replace("```", '')
    else:
        raise ValueError("No answer found in the response.")
    if Debug:
        logging.info("#"*5 + f" Translate Format")
        logging.info(f"question: {question}")
        logging.info(f"prompt_translate: {prompt_translate}")
        logging.info(f"rsp answer: {rsp}")
        logging.info(f"Formated user_input: {user_input}")
    
    logging.info(f"format result: {user_input}")
    prompt_retrieval = build_retrieval_prompt(user_input)
    rsp = qa_interface(prompt_retrieval)
    retrieval_doc = rsp['source_documents'][0]  # Document å¯¹è±¡
    logging.info(f"retrieval result: {retrieval_doc}")
    
    retrieval_pde_case = pde_case_vectordb.similarity_search(query="", k=1,filter={"uid": retrieval_doc.metadata['uid']})[0].page_content
    retrieval_pde_case = json.loads(retrieval_pde_case)
    if Debug:
        # os.path.join(os.getenv('OUTPUT_DIR'), "retrieval_pde_case.json")
        with open(os.path.join(os.getenv('OUTPUT_DIR'), "retrieval_pde_case.json"), "w", encoding="utf-8") as f:
            json.dump(retrieval_pde_case, f, ensure_ascii=False, indent=4)
        logging.info(f"retrieval_pde_case: {retrieval_pde_case}")
    
    # è§„åˆ’ä»»åŠ¡
    planning_agent = Agent(instruction=config['thinking_prompt'].strip(), role='planning agent', is_recorded=True)
    task_split_prompt = add_retrieval_case_to_prompt(question, retrieval_pde_case, config['tools_info_dict'])
    
    # è¾“å‡ºè§„åˆ’ç»“æœ
    if Debug:
        logging.info("\n" + "#"*10 + " 1 ä»»åŠ¡è§„åˆ’ " + "#"*10)
        logging.info("planning agent å·²åˆ›å»º")
        logging.info(f"Task split prompt: \n{task_split_prompt}")
    task_split_think, task_split_answer = planning_agent.chat(task_split_prompt)
    # è§£æä»»åŠ¡åˆ†è§£ç»“æœï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
    task_split_info = json.loads(task_split_answer)
    # TODO: logging info è¾“å‡ºä»»åŠ¡æ‹†åˆ†ä¿¡æ¯
    if Debug:
        logging.info(f"\nä»»åŠ¡è§„åˆ’å®Œæˆï¼š")
        logging.info(f"ğŸ¤” ä»»åŠ¡æ‹†åˆ†\nğŸ§  ä»»åŠ¡æ‹†åˆ†æ€è€ƒè¿‡ç¨‹ï¼š\n{task_split_think}")
        # logging.info(f"Task split think: \n{task_split_think}")
        logging.info(f"Task split answer: \n{task_split_answer}")
        logging.info(f"ä»»åŠ¡æ‹†åˆ†ç»“æœï¼š\n{task_split_info}")
        
        logging.info("\n" + "#"*10 + " 2 å·¥å…·è°ƒç”¨ " + "#"*10)
        logging.info("å·¥å…· agent åˆ›å»ºï¼š")
        
    # å¼€å§‹å·¥å…·ä½¿ç”¨
    implement_agent = Agent(instruction=config['implement_prompt'].strip().format(user_input=config['user_input']), role='Implement Agent', is_recorded=True)
    # é¡ºåºå¤„ç†æ¯ä¸ªä»»åŠ¡
    previous_steps_info = []  # å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨ï¼Œç”¨äºå‚æ•°ä¼ é€’
    for idx, task_info in enumerate(task_split_info):
        # å¾—åˆ°å½“å‰å¾…è°ƒç”¨å·¥å…·
        tool_name = task_info["tool_name"]
        # å¾—åˆ°å½“å‰ä»»åŠ¡æè¿°
        task_desc = task_info["reasoning"]
        # å¾—åˆ°å½“å‰å·¥å…·è¯¦æƒ…
        tool_info = [item for item in config['tools_info_dict'].items() if item[0] == tool_name]
        tool_info = tool_info[0] if tool_info else None
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·¥å…·ä¿¡æ¯ï¼Œè¾“å‡ºé”™è¯¯ä¿¡æ¯
        if not tool_info:
            # st.error("âš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            print(f"\nâš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚\n")
            logging.error(f"âš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            raise ValueError(f"Invalid tool name: {tool_name}")
        # print(f"\nå·²åœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°å·¥å…·ï¼š{tool_name}\n")
        logging.info("#"*5 + f" å·²åœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°å·¥å…·ï¼š{tool_name}")
        
        # ç¡®å®šå·¥å…·å‚æ•°
        tool_params_prompt = determine_tool_params_prompt(tool_info, previous_steps_info)
        tool_params_think, tool_params_answer = implement_agent.chat(tool_params_prompt)
        # å·¥å…·è§£æ
        tool_params = json.loads(tool_params_answer)
        tool_params_kwargs = parse_tool_params(tool_params, previous_steps_info)
        if Debug:
            logging.info(f"Tool info: {tool_info}")
            logging.info(f"\nğŸ› ï¸ å·¥å…·é€‰æ‹©ï¼š{tool_name}")
            logging.info(f"\tTool params prompt: \n{tool_params_prompt}")
            # logging.info(f"Tool params answer: \n{tool_params_answer}")
            # logging.info(f"Tool params: {tool_params}")
            # logging.info(f"Tool params kwargs: {tool_params_kwargs}")
            
            logging.info(f"\tğŸ§  ç¡®å®šå·¥å…·å‚æ•°ï¼š\n{tool_params_think}")
            logging.info(f"\tå‚æ•°ï¼š\n{tool_params_answer}")
            logging.info(f"\tè¾“å‡ºå‚æ•° json: {tool_params}")
            logging.info(f"\tå‚æ•°è§£æç»“æœï¼š{tool_params_kwargs}")
            
            logging.info(f"\nå·¥å…·{tool_name}å‚æ•°è§£æå®Œæˆ")
            # logging.info(f"å·¥å…·{tool_name}å‚æ•°ï¼š{tool_params_kwargs}")
            
        # æ‰§è¡Œå·¥å…·
        tool_result = execute_tool(config['Tool_Dict'], tool_name, tool_params_kwargs)
        
        # è®°å½•å·¥å…·æ‰§è¡Œç»“æœ
        previous_steps_info.append({
            "step": idx+1,
            "tool_name": tool_name,
            "params": tool_params_kwargs,
            **tool_result
        })
        
        if Debug:
            logging.info(f"\nå·¥å…· {tool_name} æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç»“æœï¼š\n{tool_result}")
            logging.info(f"\nâœ… æ‰§è¡Œç»“æœï¼š")
            figure_flag = False
            for id, (key, item) in enumerate(tool_result.items()):
                if isinstance(item["value"], plt.Figure):
                    figure_flag = True
                    # plt.plot(item["value"])  # æ˜¾ç¤º Matplotlib å›¾åƒ
                    # plt.show()
                    plt.figure(item["value"].figure)  # æŒ‡å®šè¦æ˜¾ç¤ºçš„å›¾å½¢
                    # plt.show()  # æ˜¾ç¤º Matplotlib å›¾åƒ
                    logging.info(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼šå›¾åƒå·²å±•ç¤º")
                    plt.savefig(os.path.join(os.getenv('OUTPUT_DIR'), f"{tool_name}_{id+1}.png"))
                else:
                    print(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼š{item['value']}")  # å…¶ä»–ç±»å‹ç›´æ¥è¾“å‡º
                    logging.info(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼š\n{item['value']}")
            if figure_flag:
                plt.show()  # æ˜¾ç¤º Matplotlib å›¾åƒ
            # logging.info(f"\n{tool_name}æ‰§è¡Œå®Œæˆ")
            logging.info(f"å½“å‰è®°å½•çš„æ‰€æœ‰å·¥å…·ç»“æœï¼š\n{previous_steps_info}")

    logging.info("\næ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ")
    return previous_steps_info, planning_agent.messages, implement_agent.messages


def process_difficult_query(question, config, Debug=False):
    # TODO: 1 retrieval module
    pde_problem_directory = "./database/pde_problem_faiss"
    pde_case_directory = "./database/pde_case_faiss"
    # local_model_path = "tbs17/MathBERT"
    local_model_path = r"C:\Users\jermain\.cache\huggingface\hub\models--tbs17--MathBERT\snapshots\e26235ccf2b14614ef278b19caa44fdb5dcf050f"
    pde_problem_vectordb = FAISS.load_local(pde_problem_directory, HuggingFaceEmbeddings(model_name=local_model_path),allow_dangerous_deserialization=True)
    pde_case_vectordb = FAISS.load_local(pde_case_directory, HuggingFaceEmbeddings(model_name=local_model_path),allow_dangerous_deserialization=True)
    
    chat_model_name = config['model_info'].get('CHAT_MODEL', '')
    temperature = 0.0
    if chat_model_name.startswith("deepseek"):
        chat_model = ChatDeepSeek(model=chat_model_name, temperature=temperature)
    elif chat_model_name.lower().startswith("gpt"):
        chat_model = ChatOpenAI(model=chat_model_name, temperature=temperature)
    else:
        print('the chat model is not identified, only the model start with deepseek and gpt can be identified.')
        sys.exit()
    
    # k = config['searchdocs']
    k = 2
    retriever = pde_problem_vectordb.as_retriever(search_type="similarity", search_kwargs={"k": k})
    qa_interface = RetrievalQA.from_chain_type(
        llm=chat_model,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True
    )
    
    prompt_translate = build_standard_format_prompt(question)
    rsp = qa_interface(prompt_translate)
    user_input = rsp['result']
    # å°†å…¶æå–å‡ºæ¥
    match = re.search(r'<answer>(.*?)</answer>(.*)', user_input, re.DOTALL)
    if match:
        user_input = match.group(1).strip().replace("json", "").replace("```", '')
    else:
        raise ValueError("No answer found in the response.")
    if Debug:
        logging.info("#"*5 + f" Translate Format")
        logging.info(f"question: {question}")
        logging.info(f"prompt_translate: {prompt_translate}")
        logging.info(f"rsp answer: {rsp}")
        logging.info(f"Formated user_input: {user_input}")
    
    logging.info(f"format result: {user_input}")
    prompt_retrieval = build_retrieval_prompt(user_input)
    rsp = qa_interface(prompt_retrieval)
    retrieval_doc = rsp['source_documents'][0]  # Document å¯¹è±¡
    logging.info(f"retrieval result: {retrieval_doc}")
    
    retrieval_pde_case = pde_case_vectordb.similarity_search(query="", k=1,filter={"uid": retrieval_doc.metadata['uid']})[0].page_content
    retrieval_pde_case = json.loads(retrieval_pde_case)
    if Debug:
        # os.path.join(os.getenv('OUTPUT_DIR'), "retrieval_pde_case.json")
        with open(os.path.join(os.getenv('OUTPUT_DIR'), "retrieval_pde_case.json"), "w", encoding="utf-8") as f:
            json.dump(retrieval_pde_case, f, ensure_ascii=False, indent=4)
        logging.info(f"retrieval_pde_case: {retrieval_pde_case}")
    
    # TODO: 2 è§„åˆ’ä»»åŠ¡
    planning_agent = Agent(instruction=config['thinking_prompt'].strip(), role='planning agent', is_recorded=True)
    task_split_prompt = add_retrieval_case_to_prompt(question, retrieval_pde_case, config['tools_info_dict'])
    
    # è¾“å‡ºè§„åˆ’ç»“æœ
    if Debug:
        logging.info("\n" + "#"*10 + " 1 ä»»åŠ¡è§„åˆ’ " + "#"*10)
        logging.info("planning agent å·²åˆ›å»º")
        logging.info(f"Task split prompt: \n{task_split_prompt}")
    task_split_think, task_split_answer = planning_agent.chat(task_split_prompt)
    # è§£æä»»åŠ¡åˆ†è§£ç»“æœï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
    task_split_info = json.loads(task_split_answer)
    # : logging info è¾“å‡ºä»»åŠ¡æ‹†åˆ†ä¿¡æ¯
    if Debug:
        logging.info(f"\nä»»åŠ¡è§„åˆ’å®Œæˆï¼š")
        logging.info(f"ğŸ¤” ä»»åŠ¡æ‹†åˆ†\nğŸ§  ä»»åŠ¡æ‹†åˆ†æ€è€ƒè¿‡ç¨‹ï¼š\n{task_split_think}")
        # logging.info(f"Task split think: \n{task_split_think}")
        logging.info(f"Task split answer: \n{task_split_answer}")
        logging.info(f"ä»»åŠ¡æ‹†åˆ†ç»“æœï¼š\n{task_split_info}")
        
        logging.info("\n" + "#"*10 + " 2 å·¥å…·è°ƒç”¨ " + "#"*10)
        logging.info("å·¥å…· agent åˆ›å»ºï¼š")
    
    # TODO: 3 å·¥å…·ä½¿ç”¨
    # åˆ†3ä¸ªæ¨¡å—ï¼ša. æ€»ç­–ç•¥ï¼Œb. PDE å¯¹è±¡æå–ï¼Œc. PINNæ±‚è§£
    # å¼€å§‹å·¥å…·ä½¿ç”¨
    if retrieval_pde_case:
        rag_case_prompt = config['rag_case_prompt'].strip().format(
            RAG_case_problem=retrieval_pde_case["problem_description"], 
            RAG_case_solution=''.join('\n' + json.dumps(retrieval_pde_case["expected_tool_chain"], ensure_ascii=False, indent=4) + '\n'))
    else:
        rag_case_prompt = ''
    # prompt
    pde_orchestrator_prompt = config['pde_orchestrator_prompt'].strip().format(user_input=user_input, task_split_procedure=task_split_answer, RAG_case_prompt=rag_case_prompt)
    pde_parser_prompt = config['pde_parser_prompt'].strip().format(user_input=user_input)
    pde_solver_prompt = config['pde_solver_prompt'].strip().format(user_input=user_input)
    
    pde_orchestrator_agent = Agent(instruction=pde_orchestrator_prompt, role='PDE Orchestrator', is_recorded=True)
    pde_parser_agent = Agent(instruction=pde_parser_prompt, role='PDE Parser', is_recorded=True)
    pde_solver_agent = Agent(instruction=pde_solver_prompt, role='PDE Solver', is_recorded=True)
    
    # è°ƒç”¨å·¥å…·
    # é¡ºåºå¤„ç†æ¯ä¸ªä»»åŠ¡
    # {step: x, agent: xxx, tool_name: xxx, "params":xxx, **tool_result}
    previous_steps_info = []  # å·¥å…·æ‰§è¡Œç»“æœåˆ—è¡¨ï¼Œç”¨äºå‚æ•°ä¼ é€’
    pde_parser_tool_list = config['pde_parser_tool']
    for idx, task_info in enumerate(task_split_info):
        # å¾—åˆ°å½“å‰å¾…è°ƒç”¨å·¥å…·
        tool_name = task_info["tool_name"]
        # å¾—åˆ°å½“å‰ä»»åŠ¡æè¿°
        task_desc = task_info["reasoning"]
        # å¾—åˆ°å½“å‰å·¥å…·è¯¦æƒ…
        tool_info = [item for item in config['tools_info_dict'].items() if item[0] == tool_name]
        tool_info = tool_info[0] if tool_info else None
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å·¥å…·ä¿¡æ¯ï¼Œè¾“å‡ºé”™è¯¯ä¿¡æ¯
        if not tool_info:
            # st.error("âš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            print(f"\nâš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚\n")
            logging.error(f"âš ï¸ æœªåœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°è¯¥å·¥å…·ï¼Œè¯·æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦æ­£ç¡®ã€‚")
            raise ValueError(f"Invalid tool name: {tool_name}")
        # print(f"\nå·²åœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°å·¥å…·ï¼š{tool_name}\n")
        logging.info("#"*5 + f" å·²åœ¨å·¥å…·åº“ä¸­æ‰¾åˆ°å·¥å…·ï¼š{tool_name}")
        
        # ç¡®å®šå·¥å…·å‚æ•°
        tool_params_prompt = determine_tool_params_prompt(tool_info, previous_steps_info)
        
        if tool_name in pde_parser_tool_list:
            implement_agent = pde_parser_agent
        else:
            implement_agent = pde_solver_agent
            
        tool_params_think, tool_params_answer = implement_agent.chat(tool_params_prompt)
        # å·¥å…·è§£æ
        tool_params = json.loads(tool_params_answer)
        tool_params_kwargs = parse_tool_params(tool_params, previous_steps_info)
        if Debug:
            logging.info(f"Tool info: {tool_info}")
            logging.info(f"\nğŸ› ï¸ å·¥å…·é€‰æ‹©ï¼š{tool_name}")
            logging.info(f"\tTool params prompt: \n{tool_params_prompt}")
            # logging.info(f"Tool params answer: \n{tool_params_answer}")
            # logging.info(f"Tool params: {tool_params}")
            # logging.info(f"Tool params kwargs: {tool_params_kwargs}")
            
            logging.info(f"\tğŸ§  ç¡®å®šå·¥å…·å‚æ•°ï¼š\n{tool_params_think}")
            logging.info(f"\tå‚æ•°ï¼š\n{tool_params_answer}")
            logging.info(f"\tè¾“å‡ºå‚æ•° json: {tool_params}")
            logging.info(f"\tå‚æ•°è§£æç»“æœï¼š{tool_params_kwargs}")
            
            logging.info(f"\nå·¥å…·{tool_name}å‚æ•°è§£æå®Œæˆ")
            # logging.info(f"å·¥å…·{tool_name}å‚æ•°ï¼š{tool_params_kwargs}")
            
        # æ‰§è¡Œå·¥å…·
        tool_result = execute_tool(config['Tool_Dict'], tool_name, tool_params_kwargs)
        
        current_step_info = {
            "step": idx+1,
            "agent": implement_agent.role,
            "tool_name": tool_name,
            "params": tool_params_kwargs,
            **tool_result
        }
        
        # TODO: 4 ç¡®å®šæœ‰æ•ˆæ€§
        # ç¡®å®šå‚æ•°æ˜¯å¦åˆé€‚
        determine_continue_prompt = build_determine_continue_prompt(current_step_info, previous_steps_info)
        determine_continue_think, determine_continue_answer = pde_orchestrator_agent.chat(determine_continue_prompt)
        # è§£æç»“æœ
        determine_continue_answer = determine_continue_answer.strip().replace("json", "").replace("```", '')
        determine_continue_result = json.loads(determine_continue_answer)
        validation = False if determine_continue_result['validation'].lower() == "invalid" else True
        if Debug:
            logging.info(f"\nğŸ§  ç¡®å®šæ˜¯å¦ç»§ç»­ï¼š\n{determine_continue_result['validation']}")
        
        turn_idx = 1
        while not validation and turn_idx < 3:
            # é‡æ–°è¾“å…¥å‚æ•°
            tool_params_prompt = redetermine_tool_params_prompt(tool_info, previous_steps_info, determine_continue_result)
            tool_params_think, tool_params_answer = implement_agent.chat(tool_params_prompt)
            # å·¥å…·è§£æ
            tool_params = json.loads(tool_params_answer)
            tool_params_kwargs = parse_tool_params(tool_params, previous_steps_info)
            if Debug:
                logging.info("#" * 5 + f" é‡æ–°ç¡®å®šå·¥å…·å‚æ•°å’Œæ‰§è¡Œç»“æœ")
            
            # æ‰§è¡Œå·¥å…·
            tool_result = execute_tool(config['Tool_Dict'], tool_name, tool_params_kwargs)
            
            current_step_info = {
                "step": idx+1,
                "agent": implement_agent.role,
                "tool_name": tool_name,
                "params": tool_params_kwargs,
                **tool_result
            }
            # ç¡®å®šå‚æ•°æ˜¯å¦åˆé€‚
            determine_continue_prompt = build_determine_continue_prompt(current_step_info, previous_steps_info)
            determine_continue_think, determine_continue_answer = pde_orchestrator_agent.chat(determine_continue_prompt)
            # è§£æç»“æœ
            determine_continue_answer = determine_continue_answer.strip().replace("json", "").replace("```", '')
            determine_continue_result = json.loads(determine_continue_answer)
            validation = False if determine_continue_result['validation'].lower() == "invalid" else True
            if Debug:
                logging.info(f"\nğŸ§  ç¡®å®šæ˜¯å¦ç»§ç»­ï¼š\n{determine_continue_result['validation']}")
                
        # è®°å½•å·¥å…·æ‰§è¡Œç»“æœ
        # previous_steps_info.append({
        #     "step": idx+1,
        #     "tool_name": tool_name,
        #     "params": tool_params_kwargs,
        #     **tool_result
        # })
        previous_steps_info.append(current_step_info)
        
        if Debug:
            logging.info(f"\nå·¥å…· {tool_name} æ‰§è¡Œå®Œæˆï¼Œè¿”å›ç»“æœï¼š\n{tool_result}")
            logging.info(f"\nâœ… æ‰§è¡Œç»“æœï¼š")
            figure_flag = False
            for id, (key, item) in enumerate(tool_result.items()):
                if isinstance(item["value"], plt.Figure):
                    figure_flag = True
                    # plt.plot(item["value"])  # æ˜¾ç¤º Matplotlib å›¾åƒ
                    # plt.show()
                    plt.figure(item["value"].figure)  # æŒ‡å®šè¦æ˜¾ç¤ºçš„å›¾å½¢
                    # plt.show()  # æ˜¾ç¤º Matplotlib å›¾åƒ
                    logging.info(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼šå›¾åƒå·²å±•ç¤º")
                    plt.savefig(os.path.join(os.getenv('OUTPUT_DIR'), f"{tool_name}_{id+1}.png"))
                else:
                    print(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼š{item['value']}")  # å…¶ä»–ç±»å‹ç›´æ¥è¾“å‡º
                    logging.info(f"\tå·¥å…·æ‰§è¡Œç»“æœ{id + 1}ï¼š\n{item['value']}")
            if figure_flag:
                plt.show()  # æ˜¾ç¤º Matplotlib å›¾åƒ
            # logging.info(f"\n{tool_name}æ‰§è¡Œå®Œæˆ")
            logging.info(f"å½“å‰è®°å½•çš„æ‰€æœ‰å·¥å…·ç»“æœï¼š\n{previous_steps_info}")

    logging.info("\næ‰€æœ‰æ­¥éª¤æ‰§è¡Œå®Œæˆ")
    return previous_steps_info, planning_agent.messages, implement_agent.messages
    
